{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVmeak2vHVDO"
   },
   "source": [
    "# Setup of Colab Environment\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBBsacrjogHF"
   },
   "source": [
    "Every Colab runs it's own instance on cloud. We need setup workshop enviroment in those steps:  \n",
    "* Setup GPU instance: Runtime ->  Change runtime type \n",
    "* Install workshop package with all requiremetns from git\n",
    "* Import all packages\n",
    "* Mount GDrive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaol7sxpHYUa"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/adamoz/colab_image_processing_workshop.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMcqQUwOoi6F"
   },
   "outputs": [],
   "source": [
    "!pip install tb-nightly\n",
    "!pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHW5G8N_HirE"
   },
   "outputs": [],
   "source": [
    "# Basic tools\n",
    "from image_processing_workshop.utils import get_image_from_url\n",
    "from collections import deque\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "from shutil import rmtree\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# Architecture of NN\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import ReLU, Tanh, Dropout, Softmax, Linear, BatchNorm1d, Conv2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn.init import xavier_uniform_, normal_\n",
    "\n",
    "# Training\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, NLLLoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# Metrics\n",
    "from image_processing_workshop.eval import get_results_df\n",
    "from image_processing_workshop.eval import get_precision\n",
    "from image_processing_workshop.eval import get_recall\n",
    "from image_processing_workshop.eval import get_rec_prec\n",
    "from image_processing_workshop.eval import get_accuracy\n",
    "from image_processing_workshop.eval import get_false_positives\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from image_processing_workshop.visual import plot_classify, plot_image\n",
    "from image_processing_workshop.visual import plot_df_examples\n",
    "from image_processing_workshop.visual import plot_coocurance_matrix\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as ipw\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvkGUM-FHjhm"
   },
   "outputs": [],
   "source": [
    "drive.mount('./drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SWuljVjHpZ6"
   },
   "outputs": [],
   "source": [
    "os.listdir('./drive/My Drive/ml_college_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuN8PUIMuixo"
   },
   "source": [
    "# Work with PyTorch Datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkedPd21uixy"
   },
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7zVM1CWuixz"
   },
   "source": [
    "### Creating of dataset\n",
    "PyTorch provides easy mechanism to work with datasets. You just need to inherit from `torch.utils.data.Dataset` and override 2 methods:\n",
    " - `__len__` in a way that len(dataset) returns the size of the dataset.\n",
    " - `__getitem__` to support the indexing such that dataset[i] can be used to get ith sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f_B7mN5uizn"
   },
   "outputs": [],
   "source": [
    "class LetterDataset(Dataset):\n",
    "    \"\"\"A-F Letter dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transform (callable, optional): Optional transformation to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.raw_data = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'letter': self.raw_data[idx]}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-8Q7Kqvuizq"
   },
   "outputs": [],
   "source": [
    "letter_dataset = LetterDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TivnJxkiuizs"
   },
   "outputs": [],
   "source": [
    "len(letter_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsuvLB_Xuizw"
   },
   "outputs": [],
   "source": [
    "letter_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-0d0tUUuizz"
   },
   "source": [
    "### Apply transformations to dataset\n",
    "We can create objects with `__call__` method applying transforamtions to data from dataset. To put more transformations together, we can use `torchvision.transforms.Compose`. PyTorch provides multiple prepared  image transformations in ``torchvision.transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8oXE2RLuiz2"
   },
   "outputs": [],
   "source": [
    "class ToLower(object):\n",
    "    def __call__(self, sample):\n",
    "        return {'letter': sample['letter'].lower()}\n",
    "\n",
    "class JoinX(object):\n",
    "    def __call__(self, sample):\n",
    "        return {'letter': sample['letter'] + 'X'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olyYnuIBuiz5"
   },
   "outputs": [],
   "source": [
    "transformations = Compose([ToLower(), JoinX()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31S5bjwQuiz8"
   },
   "outputs": [],
   "source": [
    "letter_dataset = LetterDataset(transform=transformations)\n",
    "letter_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ih9Ak3douiz-"
   },
   "source": [
    "### Sampling batches from dataset\n",
    "PyTorch provides iterator `torch.utils.data.DataLoader` for work with datasets based on `torch.utils.data.Dataset` class.   \n",
    "It enables\n",
    " - batching the data\n",
    " - shuffling the data  \n",
    " - load the data in parallel manner using multiprocessing workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yVgrBu9ui0D"
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=letter_dataset, batch_size=3, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFiF3kZkui0F"
   },
   "outputs": [],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eT12L3uHui0J"
   },
   "source": [
    "## Explore prepared dataset Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYFLIm1Iui0M"
   },
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppS_nsaWui0P"
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KllG4AZkui0Q"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=True, transform=transformations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBinxha7ui0U"
   },
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvR_6lBAui0X"
   },
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e93auZwlui0b"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qA36iLfui0f"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSJkUETyui0i"
   },
   "outputs": [],
   "source": [
    "plt.subplots_adjust(wspace=1.5, hspace=2.5)\n",
    "fig = plt.figure(figsize=(20,25))\n",
    "\n",
    "img_batch, label_batch = next(iter(train_loader))\n",
    "img_batch = img_batch.squeeze(dim=1).numpy()\n",
    "label_batch = label_batch.numpy()\n",
    "for img_id in range(100):\n",
    "    ax = plt.subplot(10, 10, img_id+1)\n",
    "    img = img_batch[img_id]\n",
    "    \n",
    "    class_id = label_batch[img_id]\n",
    "    class_name = train_dataset.classes[class_id]\n",
    "    ax.imshow(img , cmap='gray')\n",
    "    ax.set_title(class_name)\n",
    "    ax.axes.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3EicSH4ui0l"
   },
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uc3UV-7mui0o"
   },
   "outputs": [],
   "source": [
    "valid_dataset = datasets.FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=False, transform=transformations)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70VjUbXHui0q"
   },
   "outputs": [],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcoPBeLQui0s"
   },
   "outputs": [],
   "source": [
    "plot_image(valid_dataset[21][0], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hl1Fyjcui0u"
   },
   "outputs": [],
   "source": [
    "labels = valid_dataset.targets\n",
    "class_names = list(map(lambda class_id: valid_dataset.classes[class_id], labels))\n",
    "df = pd.DataFrame({'class_names': class_names, 'class_ids': labels})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTnrGsdiui0x"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "df.loc[:,'class_ids'].plot(kind='hist', width=0.5)\n",
    "ax = plt.gca()\n",
    "ax_ticks = ax.xaxis.set_ticks(np.arange(0.25, 9, 0.9))\n",
    "ax_labels = ax.xaxis.set_ticklabels(list(valid_dataset.classes), rotation=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVomdwpMui0z"
   },
   "source": [
    "# Feed Forward Neural Network on Fashion MNIST\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MigiW52Pui03"
   },
   "source": [
    "## Prepare Fashion MNIST dataset\n",
    "We want to preprocess training data, specifically to have flatten shape `(28, 28) -> 784` in `torch.Tensor` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_j_VaJihui08"
   },
   "outputs": [],
   "source": [
    "class FlattenTransform:\n",
    "    def __call__(self, sample):\n",
    "        return sample.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4fhYILOui0-"
   },
   "outputs": [],
   "source": [
    "transformations = Compose([ToTensor(), FlattenTransform()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3q4cC8eui1A"
   },
   "outputs": [],
   "source": [
    "train_dataset = FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=True, \n",
    "                             transform=transformations, \n",
    "                             target_transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "valid_dataset = FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=False, \n",
    "                             transform=transformations, \n",
    "                             target_transform=None)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLjl5vvjui1B"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kq7jXAKdui1F"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G_s5-MOui1I"
   },
   "source": [
    "## Define feed forward neural network\n",
    "In case we use `torch.nn` modules, we don't need to register tensor with `torch.nn.Parameter`.   \n",
    "\n",
    "**Important:** Don't forget to setup `.eval()` or `.train()` modes for model to enforce proper behaviour of certain layers as `torch.nn.Dropout` or `torch.nn.BatchNorm1d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRuPt4yQui1I"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE1B-Bx5ui1L"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNeuralNet, self).__init__()\n",
    "        \n",
    "        self.layer_1 = Linear(784, 10)\n",
    "        ##########################\n",
    "        # TODO: Add extra layer. #\n",
    "        ##########################\n",
    "        \n",
    "        ##########################################################\n",
    "        # TODO: Prepare batch norlmalization and dropout module. #\n",
    "        ##########################################################\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        prediction = self.layer_1(input_batch)\n",
    "        ###################################################\n",
    "        # TODO: Stack activation -> bn -> dropout layers. #\n",
    "        ###################################################\n",
    "        \n",
    "        return torch.softmax(prediction, dim=1)\n",
    "        ############################################################################################\n",
    "        # TODO Advanced: Numeric optimization                                                      #\n",
    "        #       Switch torch.softmax -> torch.log_softmax during training. Softmax leave for eval. #\n",
    "        #       Use torch.nn.NLLLoss as loss (https://pytorch.org/docs/stable/nn.html#nllloss).    #\n",
    "        #       Why is it cool?                                                                    #\n",
    "        ############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riSvKJS1ui1N"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net = FeedForwardNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYlNDydRui1P"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHM8rt5qui1R"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLREjCjxui1S"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQjmKjxIui1X"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net.eval()\n",
    "predictions = feed_forward_neural_net(images)\n",
    "feed_forward_neural_net.train()\n",
    "predictions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bURO-5C6VNU3"
   },
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZ9WLgRaVWyJ"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net = feed_forward_neural_net.cuda()\n",
    "images = images.cuda()\n",
    "labels = labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7V0au2MVXB_"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net.eval()\n",
    "predictions = feed_forward_neural_net(images)\n",
    "feed_forward_neural_net.train()\n",
    "predictions[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2I5B58Cui1a"
   },
   "source": [
    "### Optimizers and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMqYgwtLui1b"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TODO(home): Try NLLLoss with log_softmax. #\n",
    "#######################################\n",
    "loss_fce = CrossEntropyLoss()\n",
    "loss_fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbGfBFBsui1d"
   },
   "outputs": [],
   "source": [
    "loss_fce(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqlZ0eoKui1h"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# TODO: Adjust learning rate. #\n",
    "###############################\n",
    "optimizer = SGD(feed_forward_neural_net.parameters(), lr=0.5)\n",
    "\n",
    "###################################\n",
    "# TODO: Switch optimizer to Adam. #\n",
    "###################################\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVAkwkTOui1l"
   },
   "source": [
    "## Training of neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFq_EW3vui1o"
   },
   "outputs": [],
   "source": [
    "def get_valid_acc_and_loss(model, loss_fce, valid_loader):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    was_training = model.training\n",
    "    \n",
    "    model.eval()\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        predictions = model(images)\n",
    "        accuracy += (predictions.argmax(dim=1) == labels).type(torch.FloatTensor).mean().item() \n",
    "        loss += loss_fce(predictions, labels).item()\n",
    "    model.train(mode=was_training)\n",
    "    return accuracy / len(valid_loader) * 100, loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYzj8a2Xui1r"
   },
   "outputs": [],
   "source": [
    "get_valid_acc_and_loss(feed_forward_neural_net, loss_fce, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vkBqhjJui1t"
   },
   "outputs": [],
   "source": [
    "# Initial params setup.\n",
    "epochs = 2\n",
    "report_period = 100\n",
    "batch_iteration = 0\n",
    "\n",
    "# Storing of some data.\n",
    "train_leak_loss = deque(maxlen=report_period)\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "valid_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw3IvcyTui1v"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Setup net to train mode and go through one epoch.\n",
    "    feed_forward_neural_net.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        batch_iteration += 1\n",
    "        \n",
    "        ##################\n",
    "        # Training Phase #\n",
    "        ##################\n",
    "        optimizer.zero_grad()\n",
    "        predictions = feed_forward_neural_net.forward(images)\n",
    "        loss = loss_fce(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # Validation Phase #\n",
    "        ####################\n",
    "        train_leak_loss.append(loss.item())\n",
    "        if batch_iteration % report_period == 0:\n",
    "            feed_forward_neural_net.eval()\n",
    "            \n",
    "            # We don't want to collect info for gradients from here.\n",
    "            with torch.no_grad():\n",
    "                valid_accuracy, valid_loss = get_valid_acc_and_loss(feed_forward_neural_net, loss_fce, valid_loader)\n",
    "                \n",
    "            print(f\"Epoch: {epoch+1}/{epochs}.. \",\n",
    "                  f\"Train Loss: {round(np.mean(train_leak_loss), 2)}.. \",\n",
    "                  f\"Valid Loss: {round(valid_loss, 2)}.. \",\n",
    "                  f\"Valid Acc: {round(valid_accuracy, 2)}%\")\n",
    "            \n",
    "            train_loss_history.append(np.mean(train_leak_loss))\n",
    "            valid_loss_history.append(valid_loss)\n",
    "            valid_acc_history.append(valid_accuracy)\n",
    "                   \n",
    "            feed_forward_neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsuQ7GW4ui1w"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cross Entropy')\n",
    "plt.plot(train_loss_history, label='Train loss')\n",
    "plt.plot(valid_loss_history, label='Valid loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TvRn2W5ui1y"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(valid_acc_history, label='Valid acc')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Acc(%)')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wwvt6thvui10"
   },
   "source": [
    "## Results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBgbEypTui10"
   },
   "outputs": [],
   "source": [
    "feed_forward_neural_net.eval()\n",
    "feed_forward_neural_net = feed_forward_neural_net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g94OE6wFui12"
   },
   "source": [
    "### View single images and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UlP9CGpui14"
   },
   "outputs": [],
   "source": [
    "plot_classify(input_tensor=valid_dataset[12][0], \n",
    "              model=feed_forward_neural_net, image_shape=[28,28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzPId58cui16"
   },
   "source": [
    "### Load reuslts to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsjCesuXui1-"
   },
   "outputs": [],
   "source": [
    "df = get_results_df(feed_forward_neural_net, valid_loader)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOA6USVHui2A"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Prediction Score')\n",
    "df[df.label_class_name=='Dress'].label_class_score.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GxJCRqaui2C"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(df.iloc[:25], image_shape=[28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB9gp6SYui2E"
   },
   "source": [
    "### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aU4rj6_1ui2G"
   },
   "outputs": [],
   "source": [
    "get_precision(df, 'Dress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guzY10iOui2M"
   },
   "outputs": [],
   "source": [
    "get_recall(df, 'Dress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFOAG-h5ui2S"
   },
   "outputs": [],
   "source": [
    "get_rec_prec(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsNfWZsZui2W"
   },
   "outputs": [],
   "source": [
    "get_accuracy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxcv29OYui2X"
   },
   "source": [
    "### False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xZWzhCLui2b"
   },
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ej6QjwtEui2f"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(fp, image_shape=[28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-1bUhX5ui2g"
   },
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt', predicted_class_name='Pullover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnkyKyixui2i"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(fp, image_shape=[28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlFtFNT_ui2m"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iR6UrOU6ui2p"
   },
   "outputs": [],
   "source": [
    "plot_coocurance_matrix(df, use_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpeMW9lLui2q"
   },
   "source": [
    "# Convolutional Neural Network on Fashion MNIST\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yhdDBycui2u"
   },
   "source": [
    "## Intro to convolutional filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTsaTM8iui2x"
   },
   "source": [
    "### Download your favouirite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tA19Kx8Sui2x"
   },
   "outputs": [],
   "source": [
    "url = 'https://media.wired.com/photos/5bbf72c46278de2d2123485b/master/w_582,c_limit/soyuz-1051882240.jpg'\n",
    "img = get_image_from_url(url, to_grayscale=True)\n",
    "img = img / 255.\n",
    "plot_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y97FueUzui2z"
   },
   "source": [
    "### Explore prepared filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdWnc7Edui20"
   },
   "outputs": [],
   "source": [
    "initial_filter = np.array([[-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1]])\n",
    "filter_1 = initial_filter\n",
    "filter_2 = -filter_1\n",
    "filter_3 = filter_1.T\n",
    "filter_4 = -filter_3\n",
    "filters = np.array([filter_1, filter_2, filter_3, filter_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJFGDLOFui22"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "    width, height = filters[i].shape\n",
    "    \n",
    "    # Add -1 1 annotations to image.\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if filters[i][x][y]<0 else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1x3q4iCui23"
   },
   "source": [
    "### Build small network initialised with those filters\n",
    "In the examples, we will use `torch.nn.conv2d` https://pytorch.org/docs/stable/nn.html#conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjwDAeKLui25"
   },
   "outputs": [],
   "source": [
    "# In PyTorch, we have channels on 1st. So here we have 4 filters, each has 1 channel, all are shape 4x4.\n",
    "filters_torch = torch.from_numpy(filters).unsqueeze(1).type(torch.DoubleTensor)\n",
    "filters_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4KJ27Ahui27"
   },
   "outputs": [],
   "source": [
    "img_torch = torch.from_numpy(img).unsqueeze(0).unsqueeze(1)\n",
    "img_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEG8dG3iui2-"
   },
   "source": [
    "Convoluton filters efectively change height and width of input image that\n",
    "\n",
    "$H_{out} = \\lfloor \\frac{H_{in}+2×padding[0]−dilation[0]×(kernel\\_size[0]−1)−1}{stride[0]} +1 \\rfloor$   \n",
    "$W_{out} = \\lfloor \\frac{W_{in}+2×padding[1]−dilation[1]×(kernel\\_size[1]−1)−1}{stride[1]} +1 \\rfloor$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVsbdGFbui2_"
   },
   "outputs": [],
   "source": [
    "class ConvNeuralNetSimple(nn.Module):    \n",
    "    def __init__(self, filters_torch):\n",
    "        super(ConvNeuralNetSimple, self).__init__()\n",
    "        \n",
    "        height, width = filters_torch.shape[2:]\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=4, \n",
    "                                    kernel_size=(height, width), bias=False)\n",
    "        self.conv_layer.weight.data = filters_torch\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.conv_layer(images)\n",
    "    \n",
    "conv_neural_net_simple = ConvNeuralNetSimple(filters_torch)\n",
    "conv_neural_net_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiUttO1Oui3D"
   },
   "outputs": [],
   "source": [
    "img_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyibOLWfui3G"
   },
   "outputs": [],
   "source": [
    "feature_maps = conv_neural_net_simple(img_torch)\n",
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pSeYjpwKui3I"
   },
   "source": [
    "### Visualization of conv layer feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrAxg0iEui3J"
   },
   "outputs": [],
   "source": [
    "def vizualize_feature_maps(feature_maps, n_maps= 4):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    for i in range(n_maps):\n",
    "        ax = fig.add_subplot(1, n_maps, i+1, xticks=[], yticks=[])\n",
    "        # grab layer outputs\n",
    "        ax.imshow(np.squeeze(feature_maps[0,i].data.numpy()), cmap='gray')\n",
    "        ax.set_title('Output %s' % str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1gsorLLui3K"
   },
   "outputs": [],
   "source": [
    "# Source img.\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# Convolution filters.\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "\n",
    "# Feature maps.    \n",
    "vizualize_feature_maps(feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9MUUgcLui3L"
   },
   "source": [
    "### Sensitivity of image on convolution filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zafSBxzTui3O"
   },
   "outputs": [],
   "source": [
    "feature_map = feature_maps[0][0].detach().numpy()\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHFsf-X1ui3P"
   },
   "outputs": [],
   "source": [
    "plot_image(filter_1, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4YtNLkeui3R"
   },
   "outputs": [],
   "source": [
    "feature_map_max = feature_map.max()\n",
    "def plot_sensitivity(tolerance):\n",
    "    feature_map_filtered = (feature_map >= (feature_map_max - tolerance)).astype(int)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    im = plt.imshow(feature_map_filtered, cmap='gray')\n",
    "    plt.colorbar(im, orientation='horizontal')\n",
    "    plt.gca().axes.set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(plot_sensitivity, tolerance=ipw.FloatSlider(0.5, min=0, max=feature_map_max - 0.1, step=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2eFMboHui3T"
   },
   "source": [
    "## Prepare Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myFXgLH3ui3V"
   },
   "outputs": [],
   "source": [
    "transformations = Compose([ToTensor()])\n",
    "\n",
    "train_dataset = FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=True, \n",
    "                             transform=transformations, target_transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataset = FashionMNIST('./drive/My Drive/ml_college_data/dataset_fashion_mnist/', download=True, train=False, \n",
    "                             transform=transformations, target_transform=None)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BA2awOomui3W"
   },
   "outputs": [],
   "source": [
    "valid_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6mAOPZaui3Z"
   },
   "source": [
    "## Define convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDzSPpbZui3Z"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWnuPq6bui3b"
   },
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        # Variables for logging of layers shapes.\n",
    "        self.shape_conv1 = None\n",
    "\n",
    "        # 1st segment of conv with batch norm and pooling.\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(1, 32, (3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(32),\n",
    "            MaxPool2d((2, 2), stride=(2, 2)))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Add another convolution blocks (64 filters) and adjsut Linear part. #\n",
    "        #############################################################################\n",
    "        \n",
    "        # Linear output.\n",
    "        self.linear = Linear(14*14*32, 10)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.conv1(images)\n",
    "        self.shape_conv1 = x.shape\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "conv_neural_net = ConvNeuralNet()\n",
    "conv_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ll8g9R3ui3d"
   },
   "outputs": [],
   "source": [
    "valid_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ys2IXodkui3e"
   },
   "outputs": [],
   "source": [
    "info = conv_neural_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4duaYQcui3g"
   },
   "outputs": [],
   "source": [
    "conv_neural_net(valid_dataset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3l5g7W5ui3j"
   },
   "outputs": [],
   "source": [
    "info = conv_neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-VmV7jRui3l"
   },
   "outputs": [],
   "source": [
    "conv_neural_net.shape_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GE7uxt_ui3o"
   },
   "source": [
    "### Optimizers and loss function\n",
    "More on loss functions can be found here: https://pytorch.org/docs/stable/nn.html#loss-functions  \n",
    "More on optimizers can be found here: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-462VYVQui3p"
   },
   "outputs": [],
   "source": [
    "loss_fce = CrossEntropyLoss()\n",
    "loss_fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3Osf54wui3q"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(conv_neural_net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54vjoO2eui3s"
   },
   "source": [
    "## Training of neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZDN7h9wui3v"
   },
   "outputs": [],
   "source": [
    "def get_valid_acc_and_loss(model, loss_fce, valid_loader):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    was_training = model.training\n",
    "    \n",
    "    model.eval()\n",
    "    for images, labels in valid_loader:\n",
    "        predictions = model(images)\n",
    "        accuracy += (predictions.argmax(dim=1) == labels).type(torch.FloatTensor).mean().item() \n",
    "        loss += loss_fce(predictions, labels).item()\n",
    "    model.train(mode=was_training)\n",
    "    return accuracy / len(valid_loader) * 100, loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtbcY0M6ui3x"
   },
   "outputs": [],
   "source": [
    "get_valid_acc_and_loss(conv_neural_net, loss_fce, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-o8DSsvui3y"
   },
   "outputs": [],
   "source": [
    "# Initial params setup.\n",
    "epochs = 2\n",
    "report_period = 100\n",
    "batch_iteration = 0\n",
    "\n",
    "# Storing of some data.\n",
    "train_leak_loss = deque(maxlen=report_period)\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "valid_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kjEY7W7ui3z"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Setup net to train mode and go through one epoch.\n",
    "    conv_neural_net.train()\n",
    "    for images, labels in train_loader:\n",
    "        batch_iteration += 1\n",
    "        \n",
    "        ##################\n",
    "        # Training Phase #\n",
    "        ##################\n",
    "        optimizer.zero_grad()\n",
    "        predictions = conv_neural_net.forward(images)\n",
    "        loss = loss_fce(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ####################\n",
    "        # Validation Phase #\n",
    "        ####################\n",
    "        train_leak_loss.append(loss.item())\n",
    "        if batch_iteration % report_period == 0:\n",
    "            conv_neural_net.eval()\n",
    "            \n",
    "            # We don't want to collect info for gradients from here.\n",
    "            with torch.no_grad():\n",
    "                valid_accuracy, valid_loss = get_valid_acc_and_loss(conv_neural_net, loss_fce, valid_loader)\n",
    "                \n",
    "            print(f'Epoch: {epoch+1}/{epochs}.. ',\n",
    "                  f\"Train Loss: {round(np.mean(train_leak_loss), 2)}.. \",\n",
    "                  f\"Valid Loss: {round(valid_loss, 2)}.. \",\n",
    "                  f\"Valid Acc: {round(valid_accuracy, 2)}%\")\n",
    "            \n",
    "            train_loss_history.append(np.mean(train_leak_loss))\n",
    "            valid_loss_history.append(valid_loss)\n",
    "            valid_acc_history.append(valid_accuracy)\n",
    "                   \n",
    "            conv_neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2HnNvVDui30"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cross Entropy')\n",
    "plt.plot(train_loss_history, label='Train loss')\n",
    "plt.plot(valid_loss_history, label='Valid loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e5g3vECui32"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(valid_acc_history, label='Valid acc')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Acc(%)')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uz-L7KCIui33"
   },
   "source": [
    "## Results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ilOZJ_Lui34"
   },
   "outputs": [],
   "source": [
    "conv_neural_net.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HIL9fcLui35"
   },
   "source": [
    "### View single images and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvC0Bq8Fui36"
   },
   "outputs": [],
   "source": [
    "plot_classify(valid_dataset[1][0], conv_neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIESbvICui37"
   },
   "source": [
    "### Load reuslts to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95PaATohui39"
   },
   "outputs": [],
   "source": [
    "df = get_results_df(conv_neural_net, valid_loader)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2caU4qQui3-"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Prediction Score')\n",
    "df[df.label_class_name=='Dress'].label_class_score.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGHRdKrLui3_"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(df.iloc[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TBLlwEOui4B"
   },
   "source": [
    "### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-gmNADCui4C"
   },
   "outputs": [],
   "source": [
    "get_precision(df, 'Dress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82ykT61Mui4D"
   },
   "outputs": [],
   "source": [
    "get_recall(df, 'Dress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CaSfCxVui4F"
   },
   "outputs": [],
   "source": [
    "get_rec_prec(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93RF8y9cui4H"
   },
   "outputs": [],
   "source": [
    "get_accuracy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQoRUBQuui4J"
   },
   "source": [
    "### False Positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEagbiFUui4K"
   },
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CtQM2FWui4L"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfwpD0Xuui4N"
   },
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt', predicted_class_name='Pullover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdNqr5mHui4P"
   },
   "outputs": [],
   "source": [
    "plot_df_examples(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NubuC2UHui4Q"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBXpFAtcui4R"
   },
   "outputs": [],
   "source": [
    "plot_coocurance_matrix(df, use_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCK59Bqjui4S"
   },
   "source": [
    "# Tensorboard visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yil-2vyDuSUF"
   },
   "source": [
    "Tensorboar is great tool for visualizing neural networks, its' topology and for logging values during training and evaliation. It's by default prepared for networks build with tensorflow  framework. Now it's really easy to connect tensorboard with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F17ckZXuWGR"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hiTClDCWuWET"
   },
   "outputs": [],
   "source": [
    "conv_neural_net_features = torch.nn.Sequential(*list(conv_neural_net.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kleS0yxcuWCF"
   },
   "outputs": [],
   "source": [
    "conv_neural_net_features = conv_neural_net_features.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9esaRC3ufKj"
   },
   "outputs": [],
   "source": [
    "features = list()\n",
    "labels = list()\n",
    "images = list()\n",
    "idx = 0\n",
    "for img_batch, label_batch in tqdm.tqdm(valid_loader):\n",
    "    img_features = conv_neural_net_features(img_batch.cuda())\n",
    "    img_features = img_features.view(img_features.size(0), -1).cpu().detach().numpy().tolist()\n",
    "    \n",
    "    features += img_features\n",
    "    labels += list(map(lambda l: valid_dataset.classes[l], label_batch.squeeze().detach().numpy().tolist()))\n",
    "    images += img_batch.detach().numpy().tolist()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx > 15:\n",
    "      break\n",
    "    \n",
    "features = torch.tensor(np.array(features))\n",
    "images = torch.tensor(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUKjINZeuhir"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_graph(conv_neural_net_features.cpu(), iter(valid_loader).__next__()[0])\n",
    "writer.add_embedding(mat=features, metadata=labels, label_img=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9lPmAMiukcO"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CVmeak2vHVDO",
    "WuN8PUIMuixo",
    "jkedPd21uixy",
    "L7zVM1CWuixz",
    "x-0d0tUUuizz",
    "ih9Ak3douiz-",
    "eT12L3uHui0J",
    "ppS_nsaWui0P",
    "c3EicSH4ui0l",
    "UVomdwpMui0z",
    "MigiW52Pui03",
    "4G_s5-MOui1I",
    "WRuPt4yQui1I",
    "x2I5B58Cui1a",
    "sVAkwkTOui1l",
    "Wwvt6thvui10",
    "g94OE6wFui12",
    "pzPId58cui16",
    "oB9gp6SYui2E",
    "gxcv29OYui2X",
    "jlFtFNT_ui2m",
    "vpeMW9lLui2q",
    "9yhdDBycui2u",
    "lTsaTM8iui2x",
    "Y97FueUzui2z",
    "Q1x3q4iCui23",
    "pSeYjpwKui3I",
    "k9MUUgcLui3L",
    "w2eFMboHui3T",
    "a6mAOPZaui3Z",
    "fDzSPpbZui3Z",
    "3GE7uxt_ui3o",
    "54vjoO2eui3s",
    "Uz-L7KCIui33",
    "1HIL9fcLui35",
    "oIESbvICui37",
    "6TBLlwEOui4B",
    "vQoRUBQuui4J",
    "NubuC2UHui4Q",
    "OCK59Bqjui4S"
   ],
   "name": "2_path_to_convolutional_neural_networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
